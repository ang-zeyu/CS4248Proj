{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BcjBTa6G7VI"
      },
      "outputs": [],
      "source": [
        "!pip install zeugma\n",
        "!pip install contractions==0.0.18\n",
        "!pip install bs4\n",
        "!pip install lxml\n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import contractions\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "import re, time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import spacy\n",
        "import pickle\n",
        "import contractions\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from bs4 import BeautifulSoup\n",
        "from spacy.vectors import Vectors"
      ],
      "metadata": {
        "id": "9V-gIxoWHMOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "\n",
        "class GloveVectorTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, CustomVectorizer,nlp):\n",
        "        self.dim = 300\n",
        "        self.CustomVectorizer=CustomVectorizer\n",
        "        self.nlp=nlp\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        embeddings=np.zeros((X.shape[0],self.dim))\n",
        "        \n",
        "        features = self.CustomVectorizer.get_feature_names_out()\n",
        "        \n",
        "        for i in range(0, X.shape[0]):\n",
        "            feature_weights = X[i]\n",
        "            more_than_zero_idx = np.nonzero(feature_weights > 0.0)\n",
        "            \n",
        "            feature_weights = feature_weights[more_than_zero_idx]\n",
        "            present_features = features[more_than_zero_idx[1]]\n",
        "            present_features_vecs = [\n",
        "                self.nlp.vocab.get_vector(token) * (feature_weights[0, idx]) for idx, token in enumerate(present_features) if self.nlp.vocab.has_vector(token)\n",
        "            ]\n",
        "            \n",
        "            if len(present_features_vecs) > 0:\n",
        "                embeddings[i] = np.mean(present_features_vecs, axis=0)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "class CustomVectorizer(CountVectorizer):\n",
        "    def __init__(self,lemmatize, stemming, keep_punctuation, keep_whitespace, merge_entities,sw_removal, nlp, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.lemmatize=lemmatize\n",
        "        self.stemming=stemming\n",
        "        self.keep_punctuation=keep_punctuation\n",
        "        self.keep_whitespace=keep_whitespace\n",
        "        self.merge_entities=merge_entities\n",
        "        self.sw_removal=sw_removal\n",
        "        self.sw=stopwords.words(\"english\")\n",
        "        self.nlp=nlp\n",
        "    def tokenize(self,doc):\n",
        "        # Lemmatizer/Tokenizer\n",
        "        sp=self.nlp\n",
        "        ps = PorterStemmer()\n",
        "\n",
        "        # Clean text\n",
        "        case_folded=doc.lower() # Case folding\n",
        "        tags_removed = BeautifulSoup(case_folded, 'lxml').get_text() # Removed html tags\n",
        "        doc_clean = contractions.fix(tags_removed) # Expand contractions\n",
        "\n",
        "        # Merge entities\n",
        "        # sp.add_pipe(\"merge_noun_chunks\")\n",
        "        # if self.merge_entities:\n",
        "          # sp.add_pipe(sp.create_pipe('merge_entities'))\n",
        "          # Restart kernel if changed\n",
        "          # sp.add_pipe('merge_entities')\n",
        "        \n",
        "        # Tokenize\n",
        "        tokens = sp(doc_clean)\n",
        "        # Remove punctuation/whitespace\n",
        "        if (not self.keep_punctuation) or (not self.keep_whitespace):\n",
        "          tokens=[token for token in tokens if not ((token.is_punct and not self.keep_punctuation) or (token.is_space and not self.keep_whitespace))]\n",
        "        \n",
        "        if self.sw_removal:\n",
        "            tokens = [token for token in tokens if token.text not in self.sw]\n",
        "        \n",
        "        # Lemmatization/Stemming\n",
        "        if self.lemmatize:\n",
        "          tokens = [token.lemma_ for token in tokens]\n",
        "        elif self.stemming:\n",
        "          tokens = [ps.stem(token.text) for token in tokens]\n",
        "        else:\n",
        "          tokens = [token.text for token in tokens]\n",
        "        \n",
        "        return tokens\n",
        "\n",
        "    def build_analyzer(self):\n",
        "        def analyser(doc):\n",
        "            res=self.tokenize(doc)\n",
        "            return(self._word_ngrams([token for token in res]))\n",
        "        return(analyser)\n",
        "\n",
        "class Preprocessor:\n",
        "    \"\"\"\n",
        "    embedding(bool): Use glove embedding of length=25\n",
        "    tf_scaling(bool): Use term-frequency scaling\n",
        "    idf_scaling(bool): Use inverse document frequency scaling\n",
        "    sw_removal(bool): Remove stop words\n",
        "    lemmatize(bool): lemmatize\n",
        "    stemming(bool): stem\n",
        "    ngram_range(bool): values of n used when forming features\n",
        "    topk(bool): Value of k using top k feature selection. Setting a value will indicate using feature selection\n",
        "    keep_punctuation(bool): keep punctuation\n",
        "    keep_whitespace(bool): keep whitespaces\n",
        "    merge_entities(bool): Merge named entities e.g. \"Empire State Building\", \"New York Times\"\n",
        "    topic_modelling(np.array): path to data_with_topicmodellingfeatures.csv file (in CS4248_G03 drive)\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding=False, tf_scaling=False, idf_scaling=False, sw_removal=False, \n",
        "                 lemmatize=False, stemming=False, ngram_range=(1,2), topk=None, keep_punctuation=False\n",
        "                 ,keep_whitespace=False, merge_entities=False, topic_modelling=None):\n",
        "        self.tf_scaling = tf_scaling\n",
        "        self.idf_scaling = idf_scaling\n",
        "        self.sw_removal = sw_removal\n",
        "        self.lemmatize = lemmatize\n",
        "        self.stemming=stemming\n",
        "        self.topk = topk\n",
        "        self.keep_punctuation=keep_punctuation\n",
        "        self.embedding=embedding\n",
        "        self.keep_whitespace=keep_whitespace\n",
        "        self.merge_entities=merge_entities\n",
        "        self.topic_modelling=topic_modelling\n",
        "        if self.topic_modelling is not None:\n",
        "            self.tm_feats=pd.read_csv(topic_modelling).iloc[: , -6:]\n",
        "        self.nlp = spacy.load('en_core_web_lg')\n",
        "        # print(self.nlp.pipe_names)\n",
        "\n",
        "        if True:\n",
        "            if lemmatize:\n",
        "                self.nlp.add_pipe('lemmatizer')\n",
        "\n",
        "            if not merge_entities:\n",
        "                if not lemmatize:\n",
        "                    self.nlp.disable_pipes('tagger', 'attribute_ruler')\n",
        "                self.nlp.disable_pipes('parser', 'ner')\n",
        "\n",
        "        # Initialize preprocessing pipeline objects\n",
        "        self.count_vectorizer=('count vectorizer',CustomVectorizer(sw_removal=self.sw_removal, ngram_range=ngram_range,lemmatize=self.lemmatize, \n",
        "                                                                   stemming=self.stemming,keep_punctuation=self.keep_punctuation,\n",
        "                                                                   keep_whitespace=self.keep_whitespace, merge_entities=self.merge_entities,\n",
        "                                                                   nlp=self.nlp)) # Count\n",
        "        self.k_best=('chi2score',SelectKBest(chi2,k=topk)) # topk\n",
        "        self.tf_idf_transformer=('tf_transformer',TfidfTransformer(use_idf=idf_scaling)) #TF-IDF\n",
        "        \n",
        "        # Pipeline\n",
        "        steps=[]\n",
        "        steps.append(self.count_vectorizer)\n",
        "        if not self.topk is None:\n",
        "          steps.append(self.k_best)\n",
        "        if self.tf_scaling:\n",
        "          steps.append(self.tf_idf_transformer)\n",
        "        if self.embedding:\n",
        "          self.glove=('glove',GloveVectorTransformer(self.count_vectorizer[1],nlp=self.nlp))\n",
        "          steps.append(self.glove)\n",
        "        print(steps)\n",
        "        self.model = Pipeline(steps)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self.model.fit(X, y)\n",
        "    def transform(self, X):\n",
        "        res=self.model.transform(X)\n",
        "        if not self.topic_modelling is None:\n",
        "            res=np.concatenate((res, self.tm_feats), axis=1)\n",
        "        return res\n",
        "    def fit_transform(self, X, y):\n",
        "        res=self.model.fit_transform(X,y)\n",
        "        if not self.topic_modelling is None:\n",
        "            print(self.tm_feats)\n",
        "            res=np.concatenate((res, self.tm_feats), axis=1)\n",
        "        return res"
      ],
      "metadata": {
        "id": "33K90knsHbU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit if running locally\n",
        "FULLTRAIN_PATH = 'fulltrain.csv'\n",
        "BALANCED_TEST_PATH = 'balancedtest.csv'\n",
        "\n",
        "# Loading data from csv file\n",
        "data = pd.read_csv(FULLTRAIN_PATH, header=None)\n",
        "X_train = data[1]\n",
        "Y_train = data[0]\n",
        "\n",
        "# Same tokenization transforms for test.\n",
        "test_data = pd.read_csv(BALANCED_TEST_PATH, header=None)\n",
        "X_test = test_data[1]\n",
        "Y_test = test_data[0]\n",
        "\n",
        "p = Preprocessor(\n",
        "    embedding=False,\n",
        "    tf_scaling=True,\n",
        "    idf_scaling=True,\n",
        "    sw_removal=True,\n",
        "    lemmatize=True,\n",
        "    stemming=False,\n",
        "    ngram_range=(1,1),\n",
        "    topk=20000,         # 20000\n",
        "    keep_punctuation=False,\n",
        "    keep_whitespace=False,\n",
        "    merge_entities=False,\n",
        "    binary=False\n",
        ")\n",
        "print(p.nlp.pipeline)"
      ],
      "metadata": {
        "id": "0ur2aXuHHXTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_pred = None\n",
        "Y_test_pred = None\n",
        "\n",
        "train_X_vec = p.fit_transform(X_train, Y_train)\n",
        "# train_X_vec = p.fit_transform(X_test, Y_test)\n",
        "test_X_vec = p.transform(X_test)\n",
        "\n",
        "if not isinstance(train_X_vec, np.ndarray):\n",
        "    train_X_vec = train_X_vec.toarray()\n",
        "if not isinstance(test_X_vec, np.ndarray):\n",
        "    test_X_vec = test_X_vec.toarray()"
      ],
      "metadata": {
        "id": "OkDIPQsiHkm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Starting models...')\n",
        "train_X_vec.shape  # verify shape"
      ],
      "metadata": {
        "id": "4hRWfmIaHnlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score\n",
        "def evaluate(X, Y):\n",
        "    print(f'Accuracy: {accuracy_score(Y, X)}')\n",
        "    print(f'Precision: {precision_score(Y, X, average=None)}')\n",
        "    print(f'Recall: {recall_score(Y, X, average=None)}')\n",
        "    print(f'F1: {f1_score(Y, X, average=None)}')"
      ],
      "metadata": {
        "id": "8IZ6WhuGHrPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "model = LogisticRegression()\n",
        "\n",
        "# OVR\n",
        "# model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "\n",
        "# OVO\n",
        "# model = OneVsOneClassifier(LinearSVC(random_state=0))"
      ],
      "metadata": {
        "id": "9WZC2ZQWJnWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = [    \n",
        "    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
        "    'C' : np.logspace(-4, 4, 20),\n",
        "    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n",
        "    'max_iter' : [100, 1000,2500, 5000]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "-bY5D9EwJorx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = GridSearchCV(model, param_grid = param_grid, cv = 3, verbose=True, n_jobs=None)"
      ],
      "metadata": {
        "id": "wz73jWPNJpsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_clf = clf.fit(train_X_vec,Y_train)"
      ],
      "metadata": {
        "id": "gdhGwCKOJqSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_clf.best_estimator_"
      ],
      "metadata": {
        "id": "5T17alZSJrdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_pred = best_clf.predict(train_X_vec)\n",
        "Y_test_pred = best_clf.predict(test_X_vec)"
      ],
      "metadata": {
        "id": "UlR9hrYnNbHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(Y_train_pred, Y_train)\n",
        "evaluate(Y_test_pred, Y_test)"
      ],
      "metadata": {
        "id": "vV9nYqz5Jv4u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}