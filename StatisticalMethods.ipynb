{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pFvAdgtmRn2"
   },
   "source": [
    "## Running this note book\n",
    "\n",
    "There are 2 types of data collection \"scripts\" here, which are further annotated in markdown cells in the notebook:\n",
    "- \"One-off\": Runs one combination of parameters. Good for quick testing.\n",
    "- \"Combination\": Runs multiple combinations of parameters. Good for overnight testing.\n",
    "\n",
    "Other than that, simply make sure that `fulltrain.csv` and `balancedtest.csv` files from the dataset are in the same folder as this notebook. (or, you can edit the variables `FULL_TRAIN/TEST_PATH` down below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xf48xIfNJJGe"
   },
   "source": [
    "# Preprocessor class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezqdOt6CJnf4"
   },
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0Pg_y1YJ0Wj"
   },
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0Fg7PEqsJj4j"
   },
   "outputs": [],
   "source": [
    "import re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import pickle\n",
    "import contractions\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from zeugma.embeddings import EmbeddingTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "from spacy.vectors import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sxTTlTtUPY2"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyZtBWZrUsZT"
   },
   "source": [
    "## Preprocessor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wIaC86k7IpOM"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class GloveVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, CustomVectorizer,nlp):\n",
    "        self.dim = 300\n",
    "        self.CustomVectorizer=CustomVectorizer\n",
    "        self.nlp=nlp\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        embeddings=np.zeros((X.shape[0],self.dim))\n",
    "        \n",
    "        features = self.CustomVectorizer.get_feature_names_out()\n",
    "        \n",
    "        for i in range(0, X.shape[0]):\n",
    "            feature_weights = X[i]\n",
    "            more_than_zero_idx = np.nonzero(feature_weights > 0.0)\n",
    "            \n",
    "            feature_weights = feature_weights[more_than_zero_idx]\n",
    "            present_features = features[more_than_zero_idx[1]]\n",
    "            present_features_vecs = [\n",
    "                self.nlp.vocab.get_vector(token) * (feature_weights[0, idx]) for idx, token in enumerate(present_features) if self.nlp.vocab.has_vector(token)\n",
    "            ]\n",
    "            \n",
    "            if len(present_features_vecs) > 0:\n",
    "                embeddings[i] = np.mean(present_features_vecs, axis=0)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "class CustomVectorizer(CountVectorizer):\n",
    "    def __init__(self,lemmatize, stemming, keep_punctuation, keep_whitespace, merge_entities,sw_removal, nlp, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lemmatize=lemmatize\n",
    "        self.stemming=stemming\n",
    "        self.keep_punctuation=keep_punctuation\n",
    "        self.keep_whitespace=keep_whitespace\n",
    "        self.merge_entities=merge_entities\n",
    "        self.sw_removal=sw_removal\n",
    "        self.sw=stopwords.words(\"english\")\n",
    "        self.nlp=nlp\n",
    "        \n",
    "    def tokenize(self,doc):\n",
    "        # Lemmatizer/Tokenizer\n",
    "        sp=self.nlp\n",
    "        ps = PorterStemmer()\n",
    "\n",
    "        # Clean text\n",
    "        case_folded=doc.lower() # Case folding\n",
    "        tags_removed = BeautifulSoup(case_folded, 'lxml').get_text() # Removed html tags\n",
    "        doc_clean = contractions.fix(tags_removed) # Expand contractions\n",
    "\n",
    "        # Merge entities\n",
    "        # sp.add_pipe(\"merge_noun_chunks\")\n",
    "        #if self.merge_entities:\n",
    "        #  sp.add_pipe('merge_entities')\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = sp(doc_clean)\n",
    "        # Remove punctuation/whitespace\n",
    "        if (not self.keep_punctuation) or (not self.keep_whitespace):\n",
    "          tokens=[token for token in tokens if not ((token.is_punct and not self.keep_punctuation) or (token.is_space and not self.keep_whitespace))]\n",
    "        \n",
    "        if self.sw_removal:\n",
    "            tokens = [token for token in tokens if token.text not in self.sw]\n",
    "        \n",
    "        # Lemmatization/Stemming\n",
    "        if self.lemmatize:\n",
    "          tokens = [token.lemma_ for token in tokens]\n",
    "        elif self.stemming:\n",
    "          tokens = [ps.stem(token.text) for token in tokens]\n",
    "        else:\n",
    "          tokens = [token.text for token in tokens]\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def build_analyzer(self):\n",
    "        def analyser(doc):\n",
    "            res=self.tokenize(doc)\n",
    "            return(self._word_ngrams([token for token in res]))\n",
    "        return(analyser)\n",
    "\n",
    "class Preprocessor:\n",
    "    \"\"\"\n",
    "    embedding(bool): Use glove embedding of length=25\n",
    "    tf_scaling(bool): Use term-frequency scaling\n",
    "    idf_scaling(bool): Use inverse document frequency scaling\n",
    "    sw_removal(bool): Remove stop words\n",
    "    lemmatize(bool): lemmatize\n",
    "    stemming(bool): stem\n",
    "    ngram_range(bool): values of n used when forming features\n",
    "    topk(bool): Value of k using top k feature selection. Setting a value will indicate using feature selection\n",
    "    keep_punctuation(bool): keep punctuation\n",
    "    keep_whitespace(bool): keep whitespaces\n",
    "    merge_entities(bool): Merge named entities e.g. \"Empire State Building\", \"New York Times\"\n",
    "    topic_modelling(np.array): path to data_with_topicmodellingfeatures.csv file (in CS4248_G03 drive)\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding=False, tf_scaling=False, idf_scaling=False, sw_removal=False, \n",
    "                 lemmatize=False, stemming=False, ngram_range=(1,2), topk=None, keep_punctuation=False\n",
    "                 ,keep_whitespace=False, merge_entities=False, topic_modelling=None, binary=False):\n",
    "        self.tf_scaling = tf_scaling\n",
    "        self.idf_scaling = idf_scaling\n",
    "        self.sw_removal = sw_removal\n",
    "        self.lemmatize = lemmatize\n",
    "        self.stemming=stemming\n",
    "        self.topk = topk\n",
    "        self.keep_punctuation=keep_punctuation\n",
    "        self.embedding=embedding\n",
    "        self.keep_whitespace=keep_whitespace\n",
    "        self.merge_entities=merge_entities\n",
    "        self.topic_modelling=topic_modelling\n",
    "        if self.topic_modelling is not None:\n",
    "            self.tm_feats=pd.read_csv(topic_modelling).iloc[: , -6:]\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "        \n",
    "        if True:\n",
    "            if not lemmatize:\n",
    "                self.nlp.disable_pipes('lemmatizer')\n",
    "\n",
    "            if not merge_entities:\n",
    "                if not lemmatize:\n",
    "                    self.nlp.disable_pipes('tagger', 'attribute_ruler')\n",
    "                self.nlp.disable_pipes('parser', 'ner')\n",
    "\n",
    "        # Initialize preprocessing pipeline objects\n",
    "        self.count_vectorizer=('count vectorizer',CustomVectorizer(sw_removal=self.sw_removal, ngram_range=ngram_range,lemmatize=self.lemmatize, \n",
    "                                                                   stemming=self.stemming,keep_punctuation=self.keep_punctuation,\n",
    "                                                                   keep_whitespace=self.keep_whitespace, merge_entities=self.merge_entities, binary=binary,\n",
    "                                                                   nlp=self.nlp)) # Count\n",
    "        self.k_best=('chi2score',SelectKBest(chi2,k=topk)) # topk\n",
    "        self.tf_idf_transformer=('tf_transformer',TfidfTransformer(use_idf=idf_scaling)) #TF-IDF\n",
    "        \n",
    "        # Pipeline\n",
    "        steps=[]\n",
    "        steps.append(self.count_vectorizer)\n",
    "        if not self.topk is None:\n",
    "          steps.append(self.k_best)\n",
    "        if self.tf_scaling:\n",
    "          steps.append(self.tf_idf_transformer)\n",
    "        if self.embedding:\n",
    "          self.glove=('glove',GloveVectorTransformer(self.count_vectorizer[1],nlp=self.nlp))\n",
    "          steps.append(self.glove)\n",
    "        print(steps)\n",
    "        self.model = Pipeline(steps)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self.model.fit(X, y)\n",
    "    def transform(self, X):\n",
    "        res=self.model.transform(X)\n",
    "        if not self.topic_modelling is None:\n",
    "            res=np.concatenate((res, self.tm_feats), axis=1)\n",
    "        return res\n",
    "    def fit_transform(self, X, y):\n",
    "        res=self.model.fit_transform(X,y)\n",
    "        if not self.topic_modelling is None:\n",
    "            print(self.tm_feats)\n",
    "            res=np.concatenate((res, self.tm_feats), axis=1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit the preprocessor / feature engineering scheme settings for the \"One-off\" scripts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rTpTcMIYuYO7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('count vectorizer', CustomVectorizer(keep_punctuation=False, keep_whitespace=False, lemmatize=True,\n",
      "                 merge_entities=False,\n",
      "                 nlp=<spacy.lang.en.English object at 0x000001E142448850>,\n",
      "                 stemming=False, sw_removal=True)), ('chi2score', SelectKBest(k=20000, score_func=<function chi2 at 0x000001E140BAD0D0>)), ('tf_transformer', TfidfTransformer())]\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x000001E142182DC0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x000001E1421822E0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x000001E15AB4E880>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x000001E15AB40100>)]\n"
     ]
    }
   ],
   "source": [
    "# Edit if running locally\n",
    "FULLTRAIN_PATH = 'fulltrain.csv'\n",
    "BALANCED_TEST_PATH = 'balancedtest.csv'\n",
    "\n",
    "# Loading data from csv file\n",
    "data = pd.read_csv(FULLTRAIN_PATH, header=None)\n",
    "X_train = data[1]\n",
    "Y_train = data[0]\n",
    "\n",
    "# Same tokenization transforms for test.\n",
    "test_data = pd.read_csv(BALANCED_TEST_PATH, header=None)\n",
    "X_test = test_data[1]\n",
    "Y_test = test_data[0]\n",
    "\n",
    "p = Preprocessor(\n",
    "    embedding=False,\n",
    "    tf_scaling=True,\n",
    "    idf_scaling=True,\n",
    "    sw_removal=True,\n",
    "    lemmatize=True,\n",
    "    stemming=False,\n",
    "    ngram_range=(1,1),\n",
    "    topk=20000,         # 20000\n",
    "    keep_punctuation=False,\n",
    "    keep_whitespace=False,\n",
    "    merge_entities=False,\n",
    "    binary=False\n",
    ")\n",
    "print(p.nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rTpTcMIYuYO7"
   },
   "outputs": [],
   "source": [
    "Y_train_pred = None\n",
    "Y_test_pred = None\n",
    "\n",
    "train_X_vec = p.fit_transform(X_train, Y_train)\n",
    "# train_X_vec = p.fit_transform(X_test, Y_test)\n",
    "test_X_vec = p.transform(X_test)\n",
    "\n",
    "if not isinstance(train_X_vec, np.ndarray):\n",
    "    train_X_vec = train_X_vec.toarray()\n",
    "if not isinstance(test_X_vec, np.ndarray):\n",
    "    test_X_vec = test_X_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GVTeoS4Nq3uH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting models...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48854, 20000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Starting models...')\n",
    "train_X_vec.shape  # verify shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nN5MmPYSkaJ3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "def evaluate(X, Y):\n",
    "    print(f'Accuracy: {accuracy_score(Y, X)}')\n",
    "    print(f'Precision: {precision_score(Y, X, average=None)}')\n",
    "    print(f'Recall: {recall_score(Y, X, average=None)}')\n",
    "    print(f'F1: {f1_score(Y, X, average=None)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"One-off\" scripts\n",
    "\n",
    "The three cells below are the mentioned \"one-off\" scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gj_RrkvFq-TG"
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Statistical models\n",
    "\n",
    "# model 1:-\n",
    "# Using linear support vector classifier\n",
    "\n",
    "# GridSearchCV causes OOm\n",
    "\n",
    "for C in [0.01, 0.05, 0.1, 0.3, 0.5, 0.75, 1, 1.5]:#, 2.0, 2.5, 5.0, 10.0]:\n",
    "    Y_train_pred = None\n",
    "    Y_test_pred = None\n",
    "\n",
    "    # Manually set parameters\n",
    "    lsvc = LinearSVC(max_iter=100000, dual=False, C=C, penalty='l2')\n",
    "\n",
    "    # training the model\n",
    "    lsvc.fit(train_X_vec, Y_train)\n",
    "    Y_train_pred = lsvc.predict(train_X_vec)\n",
    "    Y_test_pred = lsvc.predict(test_X_vec)\n",
    "\n",
    "    print(C)\n",
    "    evaluate(Y_train_pred, Y_train)\n",
    "    evaluate(Y_test_pred, Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnZkYYvGq_J_"
   },
   "outputs": [],
   "source": [
    "# model 2:-\n",
    "# Using Naive Bayes\n",
    "Y_train_pred = None\n",
    "Y_test_pred = None\n",
    "\n",
    "gnb = GaussianNB(var_smoothing=1e-9)\n",
    "gnb.fit(train_X_vec, Y_train)\n",
    "Y_train_pred = gnb.predict(train_X_vec)\n",
    "Y_test_pred = gnb.predict(test_X_vec)\n",
    "evaluate(Y_train_pred, Y_train)\n",
    "evaluate(Y_test_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnZkYYvGq_J_"
   },
   "outputs": [],
   "source": [
    "alphas = [1e-10, 0.05, 0.1, 0.25, 0.5, 1.0, 1.25, 1.5]\n",
    "\n",
    "Y_train_pred = None\n",
    "Y_test_pred = None\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(alpha)\n",
    "    mnb = MultinomialNB(alpha=alpha)\n",
    "    mnb.fit(train_X_vec, Y_train)\n",
    "    Y_train_pred = mnb.predict(train_X_vec)\n",
    "    Y_test_pred = mnb.predict(test_X_vec)\n",
    "    evaluate(Y_train_pred, Y_train)\n",
    "    evaluate(Y_test_pred, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination \"script\"\n",
    "\n",
    "This cell simply provides a convenient \"script\" to run multiple combinations of parameters at once. (good for overnight testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "FULLTRAIN_PATH = 'fulltrain.csv'\n",
    "BALANCED_TEST_PATH = 'balancedtest.csv'\n",
    "\n",
    "params = [\n",
    "    dict(\n",
    "        embedding=False,\n",
    "        tf_scaling=False,\n",
    "        idf_scaling=False,\n",
    "        sw_removal=False,\n",
    "        lemmatize=False,\n",
    "        stemming=False,\n",
    "        ngram_range=(1,1),\n",
    "        topk=10000,         # 20000\n",
    "        keep_punctuation=True,\n",
    "        keep_whitespace=False,\n",
    "        merge_entities=False\n",
    "    ),\n",
    "    dict(\n",
    "        embedding=False,\n",
    "        tf_scaling=False,\n",
    "        idf_scaling=False,\n",
    "        sw_removal=True,\n",
    "        lemmatize=False,\n",
    "        stemming=False,\n",
    "        ngram_range=(1,1),\n",
    "        topk=10000,         # 20000\n",
    "        keep_punctuation=True,\n",
    "        keep_whitespace=False,\n",
    "        merge_entities=False\n",
    "    ),\n",
    "    dict(\n",
    "        embedding=False,\n",
    "        tf_scaling=False,\n",
    "        idf_scaling=False,\n",
    "        sw_removal=True,\n",
    "        lemmatize=False,\n",
    "        stemming=False,\n",
    "        ngram_range=(1,1),\n",
    "        topk=10000,         # 20000\n",
    "        keep_punctuation=False,\n",
    "        keep_whitespace=False,\n",
    "        merge_entities=False\n",
    "    ),\n",
    "    dict(\n",
    "        embedding=False,\n",
    "        tf_scaling=False,\n",
    "        idf_scaling=False,\n",
    "        sw_removal=True,\n",
    "        lemmatize=True,\n",
    "        stemming=False,\n",
    "        ngram_range=(1,1),\n",
    "        topk=10000,         # 20000\n",
    "        keep_punctuation=False,\n",
    "        keep_whitespace=False,\n",
    "        merge_entities=False\n",
    "    ),\n",
    "    dict(\n",
    "        embedding=False,\n",
    "        tf_scaling=False,\n",
    "        idf_scaling=False,\n",
    "        sw_removal=True,\n",
    "        lemmatize=False,\n",
    "        stemming=True,\n",
    "        ngram_range=(1,1),\n",
    "        topk=10000,         # 20000\n",
    "        keep_punctuation=False,\n",
    "        keep_whitespace=False,\n",
    "        merge_entities=False\n",
    "    ),\n",
    "    dict(\n",
    "        embedding=False,\n",
    "        tf_scaling=False,\n",
    "        idf_scaling=False,\n",
    "        sw_removal=True,\n",
    "        lemmatize=False,\n",
    "        stemming=False,\n",
    "        ngram_range=(1,1),\n",
    "        topk=10000,         # 20000\n",
    "        keep_punctuation=False,\n",
    "        keep_whitespace=False,\n",
    "        merge_entities=True\n",
    "    )\n",
    "]\n",
    "\n",
    "for param_dict in params:\n",
    "\n",
    "    # Loading data from csv file\n",
    "    data = pd.read_csv(FULLTRAIN_PATH, header=None)\n",
    "    X_train = data[1]\n",
    "    Y_train = data[0]\n",
    "\n",
    "    # Same tokenization transforms for test.\n",
    "    test_data = pd.read_csv(BALANCED_TEST_PATH, header=None)\n",
    "    X_test = test_data[1]\n",
    "    Y_test = test_data[0]\n",
    "\n",
    "    print(param_dict)\n",
    "    p = Preprocessor(**param_dict)\n",
    "    print(p.nlp.pipeline)\n",
    "\n",
    "    Y_train_pred = None\n",
    "    Y_test_pred = None\n",
    "\n",
    "    train_X_vec = p.fit_transform(X_train, Y_train)\n",
    "    # train_X_vec = p.fit_transform(X_test, Y_test)\n",
    "    test_X_vec = p.transform(X_test)\n",
    "\n",
    "    if not isinstance(train_X_vec, np.ndarray):\n",
    "        train_X_vec = train_X_vec.toarray()\n",
    "    if not isinstance(test_X_vec, np.ndarray):\n",
    "        test_X_vec = test_X_vec.toarray()\n",
    "        \n",
    "    print('SVC')\n",
    "    \n",
    "    # model 1:-\n",
    "    # Using linear support vector classifier\n",
    "\n",
    "    # GridSearchCV causes OOm\n",
    "\n",
    "    for C in [0.1, 0.3, 0.5, 0.75, 1, 1.5]:#, 2.0, 2.5, 5.0, 10.0]:\n",
    "        Y_train_pred = None\n",
    "        Y_test_pred = None\n",
    "\n",
    "        # Manually set parameters\n",
    "        lsvc = LinearSVC(max_iter=100000, dual=False, C=C, penalty='l2')\n",
    "\n",
    "        # training the model\n",
    "        lsvc.fit(train_X_vec, Y_train)\n",
    "        Y_train_pred = lsvc.predict(train_X_vec)\n",
    "        Y_test_pred = lsvc.predict(test_X_vec)\n",
    "\n",
    "        print(C)\n",
    "        evaluate(Y_train_pred, Y_train)\n",
    "        evaluate(Y_test_pred, Y_test)\n",
    "    \n",
    "    # Switch to turn off NB\n",
    "    if True:\n",
    "        # model 2:-\n",
    "        # Using Naive Bayes\n",
    "        print('GaussianNB')\n",
    "        Y_train_pred = None\n",
    "        Y_test_pred = None\n",
    "\n",
    "        gnb = GaussianNB(var_smoothing=1e-9)\n",
    "        gnb.fit(train_X_vec, Y_train)\n",
    "        Y_train_pred = gnb.predict(train_X_vec)\n",
    "        Y_test_pred = gnb.predict(test_X_vec)\n",
    "        evaluate(Y_train_pred, Y_train)\n",
    "        evaluate(Y_test_pred, Y_test)\n",
    "\n",
    "\n",
    "        alphas = [1e-10, 0.05, 0.1, 0.25, 0.5, 1.0]\n",
    "\n",
    "        Y_train_pred = None\n",
    "        Y_test_pred = None\n",
    "\n",
    "        for alpha in alphas:\n",
    "            print(alpha)\n",
    "            mnb = MultinomialNB(alpha=alpha)\n",
    "            mnb.fit(train_X_vec, Y_train)\n",
    "            Y_train_pred = mnb.predict(train_X_vec)\n",
    "            Y_test_pred = mnb.predict(test_X_vec)\n",
    "            evaluate(Y_train_pred, Y_train)\n",
    "            evaluate(Y_test_pred, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below this cell are things we \"tried\"\n",
    "\n",
    "But decided ultimately not to analyse further in the report\n",
    "- Non linear SVMs - severe performance issues\n",
    "- Random forest / decision trees - while interesting, we think what we want to investigate here is largely done by Naive Bayes alone (conditional independence of features). Naive Bayes uses multiplication of likelihoods, thus, it also at least **somewhat** mimics how a decision tree favours \"distinctive\" (most reduction in entropy) features first.\n",
    "- Vector Space Model (Cosine Classifier) - this is by and large a conditionally independent model, analysable by Naive Bayes as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-linear basis SVM\n",
    "# Very, very slow! O(samples^2 + n_features)\n",
    "# Still running at 6h....\n",
    "\n",
    "USE_GRID_SEARCH_CV=False\n",
    "if USE_GRID_SEARCH_CV:\n",
    "    params = {'C':[10e-3, 10e-2, 0.1, 1,10], 'kernel':('poly', 'rbf', 'sigmoid'), 'decision_function_shape':('ovr', 'ovo')}\n",
    "    svc = GridSearchCV(SVC(max_iter=100000), params)\n",
    "else:\n",
    "    # Manually set parameters\n",
    "    svc = SVC(max_iter=100000)\n",
    "\n",
    "# Train    \n",
    "svc.fit(train_X_vec, Y_train)\n",
    "Y_train_pred = lsvc.predict(train_X_vec)\n",
    "Y_test_pred = lsvc.predict(test_X_vec)\n",
    "\n",
    "if USE_GRID_SEARCH_CV:\n",
    "    print(svc.cv_results_)\n",
    "    print(svc.best_params_)\n",
    "\n",
    "evaluate(Y_train_pred, Y_train)\n",
    "evaluate(Y_test_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfhVW5dWq_QX"
   },
   "outputs": [],
   "source": [
    "# model 3:-\n",
    "# Random Forest Classifier\n",
    "USE_GRID_SEARCH_CV=False\n",
    "if USE_GRID_SEARCH_CV:\n",
    "    params = {'n_estimators':[10,50,100,150,200,500],'criterion':('gini', 'entropy'), 'max_depth':[k for k in range(1,21)]}\n",
    "    rf = GridSearchCV(RandomForestClassifier(random_state=7), params)\n",
    "else:\n",
    "    rf = RandomForestClassifier(random_state=7)\n",
    "\n",
    "# Train, predict\n",
    "rf.fit(train_X_vec, Y_train)\n",
    "Y_train_pred = rf.predict(train_X_vec)\n",
    "Y_test_pred = rf.predict(test_X_vec)\n",
    "\n",
    "if USE_GRID_SEARCH_CV:\n",
    "    print(rf.cv_results_)\n",
    "    print(rf.best_params_)\n",
    "\n",
    "evaluate(Y_train_pred, Y_train)\n",
    "evaluate(Y_test_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZYEkamXq_TL"
   },
   "outputs": [],
   "source": [
    "print(rf.cv_results_)\n",
    "print(rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QRQ_PzlrIyq"
   },
   "outputs": [],
   "source": [
    "# model 4:-\n",
    "# Vector Space Model (Cosine) Classifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "class VSMClf:\n",
    "    def __init__(self, top_k=None):\n",
    "        self.top_k = top_k\n",
    "        self.labels = None\n",
    "        self.mean_feature_vecs = []\n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        num_features = X_train.shape[1]\n",
    "\n",
    "        Y_train = Y_train.to_numpy()\n",
    "\n",
    "        self.labels = np.unique(Y_train)\n",
    "        self.mean_feature_vecs = []\n",
    "        for i in range(0, self.labels.shape[0]):\n",
    "            label_indices = np.argwhere(Y_train == self.labels[i])\n",
    "            label_features = X_train[label_indices]\n",
    "            self.mean_feature_vecs.append(np.mean(label_features, axis=0))\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        similarities = []\n",
    "        for mean_feature_vec in self.mean_feature_vecs:\n",
    "            similarities.append(cosine_similarity(X_test, mean_feature_vec))\n",
    "        similarities = np.array(similarities)\n",
    "\n",
    "        predicted_class_indices = np.argmax(similarities, axis=0)\n",
    "        predicted_classes = self.labels[predicted_class_indices]\n",
    "\n",
    "        return predicted_classes\n",
    "\n",
    "vsm = VSMClf(top_k=10)\n",
    "vsm.fit(train_X_vec, Y_train)\n",
    "Y_train_pred = vsm.predict(train_X_vec)\n",
    "Y_test_pred = vsm.predict(test_X_vec)\n",
    "\n",
    "evaluate(Y_train_pred, Y_train)\n",
    "evaluate(Y_test_pred, Y_test)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
